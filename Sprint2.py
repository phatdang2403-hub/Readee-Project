# -*- coding: utf-8 -*-
# === 1. KHAI B√ÅO TH∆Ø VI·ªÜN ===
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import warnings

# Th∆∞ vi·ªán Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss

# === 2. C·∫§U H√åNH & H·∫∞NG S·ªê ===
warnings.filterwarnings('ignore')
sns.set_style('darkgrid')

# T√™n file d·ªØ li·ªáu
FILE_LABELED_TEXTS = "dataset/cefr_leveled_texts.csv"
FILE_WORDS_CEFR = "dataset/ENGLISH_CERF_WORDS.csv"
FILE_STORIES = "dataset/stories.csv"

# H·∫±ng s·ªë cho data processing
STORY_QUANTILE_FILTER = 0.75  # L·ªçc truy·ªán d√†i (gi·ªØ 75% ng·∫Øn nh·∫•t)
MIN_WORDS_IN_DICT = 5  # S·ªë t·ª´ t·ªëi thi·ªÉu trong t·ª´ ƒëi·ªÉn ƒë·ªÉ g√°n nh√£n
MIN_TEXT_LENGTH = 20  # ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa vƒÉn b·∫£n (k√Ω t·ª±)

# H·∫±ng s·ªë cho model training
RANDOM_STATE = 42
TFIDF_MAX_FEATURES = 5000
SGD_N_EPOCHS = 50
SGD_LEARNING_RATE = 0.01

# Target labels
TARGET_LABELS = ['A1', 'A2', 'B1', 'B2']
LEVEL_SCORES = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6}

# === 3. T·∫¢I T√ÄI NGUY√äN NLTK ===
print("ƒêang t·∫£i t√†i nguy√™n NLTK (punkt_tab, stopwords)...")
nltk.download('punkt_tab', quiet=True)
nltk.download('stopwords', quiet=True)
ENGLISH_STOP_WORDS = set(stopwords.words('english'))
print("T·∫£i NLTK ho√†n t·∫•t.")


# %%
# =============================================================================
# PH·∫¶N 2: T·∫¢I D·ªÆ LI·ªÜU
# =============================================================================

print("ƒêang t·∫£i 3 b·ªô d·ªØ li·ªáu...")
try:
    # 1. T·∫£i kho ng·ªØ li·ªáu c√≥ nh√£n
    df_cefr_texts = pd.read_csv(FILE_LABELED_TEXTS)
    print(f"T·∫£i th√†nh c√¥ng '{FILE_LABELED_TEXTS}' ({len(df_cefr_texts)} d√≤ng)")

    # 2. T·∫£i t·ª´ ƒëi·ªÉn CEFR
    df_words_cefr = pd.read_csv(FILE_WORDS_CEFR)
    print(f"T·∫£i th√†nh c√¥ng '{FILE_WORDS_CEFR}' ({len(df_words_cefr)} d√≤ng)")

    # 3. T·∫£i kho truy·ªán ng·∫Øn
    df_stories = pd.read_csv(FILE_STORIES, encoding='latin1')
    print(f"T·∫£i th√†nh c√¥ng '{FILE_STORIES}' ({len(df_stories)} d√≤ng)")

except FileNotFoundError as e:
    print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y t·ªáp {e.filename}. Vui l√≤ng ƒë·∫£m b·∫£o 3 t·ªáp ·ªü c√πng th∆∞ m·ª•c.")
except Exception as e:
    print(f"L·ªñI: {e}")

# %%
# ==========================================================
# 1: PH√ÇN T√çCH D·ªÆ LI·ªÜU KH√ÅM PH√Å (EDA)
# ==========================================================
print("\n--- 1: EDA ---")

# %%
# --- 1.1. EDA: Kho ng·ªØ li·ªáu Truy·ªán ng·∫Øn (stories.csv) ---
print("\n[EDA 1.1] Ph√¢n t√≠ch Kho ng·ªØ li·ªáu (Truy·ªán ng·∫Øn)...")

# T√≠nh ƒë·ªô d√†i (s·ªë t·ª´)
df_stories['word_count'] = df_stories['content'].apply(lambda x: len(str(x).split()))
print(df_stories['word_count'].describe())

# Tr·ª±c quan h√≥a Ph√¢n b·ªï ƒê·ªô d√†i truy·ªán
plt.figure(figsize=(10, 5))
sns.histplot(df_stories['word_count'], kde=True, bins=50)
plt.title('Ph√¢n b·ªï ƒê·ªô d√†i Truy·ªán (s·ªë t·ª´)')
plt.xlabel('S·ªë t·ª´')
plt.ylabel('T·∫ßn su·∫•t')
plt.show()

# Tr·ª±c quan h√≥a Word Cloud
print("ƒêang t·∫°o Word Cloud cho truy·ªán ng·∫Øn...")
all_stories_text = " ".join(filter(None, df_stories['content']))
wordcloud = WordCloud(width=1000, height=400,
                      background_color='white',
                      stopwords=ENGLISH_STOP_WORDS).generate(all_stories_text)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud - C√°c t·ª´ ph·ªï bi·∫øn nh·∫•t trong truy·ªán')
plt.show()

# %%
# --- 1.2. EDA: D·ªØ li·ªáu CEFR c√≥ nh√£n (cefr_leveled_texts.csv) ---
print("\n[EDA 1.2] Ph√¢n t√≠ch D·ªØ li·ªáu CEFR (VƒÉn b·∫£n c√≥ nh√£n)...")

# Chu·∫©n h√≥a nh√£n (v√≠ d·ª• 'B1' v√† 'b1' l√† m·ªôt)
df_cefr_texts['label'] = df_cefr_texts['label'].str.upper()
print("Ph√¢n b·ªï nh√£n CEFR (vƒÉn b·∫£n c√≥ nh√£n):")
print(df_cefr_texts['label'].value_counts().sort_index())

# Tr·ª±c quan h√≥a Ph√¢n b·ªï nh√£n (ph√°t hi·ªán m·∫•t c√¢n b·∫±ng)
plt.figure(figsize=(10, 5))
sns.countplot(x='label', data=df_cefr_texts, order=['A1', 'A2', 'B1', 'B2', 'C1', 'C2'])
plt.title('Ph√¢n b·ªï D·ªØ li·ªáu theo Nh√£n CEFR (VƒÉn b·∫£n c√≥ nh√£n)')
plt.xlabel('C·∫•p ƒë·ªô CEFR')
plt.ylabel('S·ªë l∆∞·ª£ng m·∫´u')
plt.show()

# %%
# --- 1.3. EDA: D·ªØ li·ªáu T·ª´ v·ª±ng CEFR (ENGLISH_CERF_WORDS.csv) ---
print("\n[EDA 1.3] Ph√¢n t√≠ch D·ªØ li·ªáu (T·ª´ v·ª±ng CEFR)...")

# Chu·∫©n h√≥a nh√£n
df_words_cefr['CEFR'] = df_words_cefr['CEFR'].str.upper()
print("Ph√¢n b·ªï nh√£n CEFR (t·ª´ v·ª±ng):")
print(df_words_cefr['CEFR'].value_counts().sort_index())

# Tr·ª±c quan h√≥a
plt.figure(figsize=(10, 5))
sns.countplot(x='CEFR', data=df_words_cefr, order=['A1', 'A2', 'B1', 'B2', 'C1', 'C2'])
plt.title('Ph√¢n b·ªï D·ªØ li·ªáu theo Nh√£n CEFR (T·ª´ v·ª±ng)')
plt.xlabel('C·∫•p ƒë·ªô CEFR')
plt.ylabel('S·ªë l∆∞·ª£ng t·ª´')
plt.show()

# %%
# --- 1.4. EDA: Ph√°t hi·ªán Ch·∫•t l∆∞·ª£ng D·ªØ li·ªáu ---
print("\n[EDA 1.4] Ph√°t hi·ªán Ch·∫•t l∆∞·ª£ng D·ªØ li·ªáu...")
# D·ªØ li·ªáu noise ƒë√£ th·∫•y trong file cefr_leveled_texts: 'Hi!\n', '-LRB-'
noise_patterns = r'(-lrb-|-rrb-)|(\n)|(i¬ø)|(&nbsp;)|(<.*?>)'
df_cefr_texts['noise_found'] = df_cefr_texts['text'].str.contains(noise_patterns, na=False, case=False)
print(f"T√¨m th·∫•y {df_cefr_texts['noise_found'].sum()} m·∫´u vƒÉn b·∫£n ch·ª©a noise (v√≠ d·ª•: \\n, -LRB-).")

print("--- K·∫æT TH√öC EDA ---")


# %%
# =============================================================================
# 2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU (DATA PREPARATION)
# =============================================================================
print("\n--- 2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU ---")

# %%
# --- 2.1. H√†m l√†m s·∫°ch vƒÉn b·∫£n ---
def clean_text(text):
    if not isinstance(text, str):
        return ""
    
    # 1. X√≥a ti√™u ƒë·ªÅ/ch√¢n trang Gutenberg
    text = re.sub(r'\*\*\*.*?\*\*\*', ' ', text)
    
    # 2. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    text = text.lower()
    
    # 3. Lo·∫°i b·ªè c√°c noise
    text = re.sub(r'<.*?>', ' ', text)  # HTML tags
    text = re.sub(r'&nbsp;', ' ', text)  # &nbsp;
    text = re.sub(r'\n', ' ', text)  # K√Ω t·ª± xu·ªëng d√≤ng
    text = re.sub(r'(-lrb-|-rrb-)', ' ', text)  # -LRB-, -RRB-
    text = re.sub(r'i¬ø', '', text)  # K√Ω t·ª± i¬ø
    
    # 4. Ch·ªâ gi·ªØ ch·ªØ c√°i, kho·∫£ng tr·∫Øng, d·∫•u '
    text = re.sub(r'[^a-z\s\']', ' ', text)
    
    # 5. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# %%
# --- 2.2. X√¢y d·ª±ng t·ª´ ƒëi·ªÉn tra c·ª©u CEFR ---
print("ƒêang x√¢y d·ª±ng b·ªô tra c·ª©u t·ª´ v·ª±ng CEFR...")
df_words_cefr['word_clean'] = df_words_cefr['headword'].apply(clean_text)
df_words_cefr['level_clean'] = df_words_cefr['CEFR'].str.upper()
word_level_map = pd.Series(
    df_words_cefr['level_clean'].values,
    index=df_words_cefr.word_clean
).to_dict()
print(f"ƒê√£ t·∫°o t·ª´ ƒëi·ªÉn tra c·ª©u v·ªõi {len(word_level_map)} t·ª´ v·ª±ng.")

# %%
# --- 2.3. H√†m g√°n nh√£n CEFR cho vƒÉn b·∫£n ---
def get_text_cefr_level(text, word_map):
    words = word_tokenize(text)
    score = 0
    word_count = 0

    for word in words:
        if word in word_map:
            level = word_map.get(word)
            if level in LEVEL_SCORES:
                score += LEVEL_SCORES[level]
                word_count += 1

    if word_count < MIN_WORDS_IN_DICT:
        return 'UNKNOWN'

    avg_score = score / word_count
    if avg_score < 1.8: return 'A1'
    if avg_score < 2.8: return 'A2'
    if avg_score < 3.8: return 'B1'
    if avg_score < 4.8: return 'B2'
    if avg_score < 5.8: return 'C1'
    return 'C2'

# %%
# --- 2.4. √Åp d·ª•ng l√†m s·∫°ch v√† h·ª£p nh·∫•t d·ªØ li·ªáu ---
print("ƒêang √°p d·ª•ng l√†m s·∫°ch v√† h·ª£p nh·∫•t 2 kho ng·ªØ li·ªáu...")

# L·ªçc truy·ªán qu√° d√†i (outliers)
upper_limit = df_stories['word_count'].quantile(STORY_QUANTILE_FILTER)
print(f"L·ªçc truy·ªán ng·∫Øn: Gi·ªØ l·∫°i c√°c truy·ªán c√≥ √≠t h∆°n {upper_limit:.0f} t·ª´ (m·ªëc {STORY_QUANTILE_FILTER*100:.0f}%).")
df_stories_filtered = df_stories[df_stories['word_count'] <= upper_limit].copy()
print(f"S·ªë l∆∞·ª£ng truy·ªán ng·∫Øn c√≤n l·∫°i sau khi l·ªçc: {len(df_stories_filtered)}")

# 1. X·ª≠ l√Ω vƒÉn b·∫£n c√≥ nh√£n (CEFR texts)
df_cefr_texts['text_clean'] = df_cefr_texts['text'].apply(clean_text)
df_cefr_texts['label_clean'] = df_cefr_texts['label'].str.upper()
df1 = df_cefr_texts[['text_clean', 'label_clean']]

# 2. X·ª≠ l√Ω truy·ªán ng·∫Øn (stories)
df_stories_filtered['text_clean'] = df_stories_filtered['content'].apply(clean_text)
print("ƒêang g√°n nh√£n CEFR cho truy·ªán ng·∫Øn...")
df_stories_filtered['label_clean'] = df_stories_filtered['text_clean'].apply(
    lambda x: get_text_cefr_level(x, word_level_map)
)
df2 = df_stories_filtered[['text_clean', 'label_clean']]
print("G√°n nh√£n truy·ªán ng·∫Øn ho√†n t·∫•t.")

# 3. H·ª£p nh·∫•t hai ngu·ªìn d·ªØ li·ªáu
final_data = pd.concat([df1, df2], ignore_index=True)

# 4. X·ª≠ l√Ω sau h·ª£p nh·∫•t
final_data = final_data.dropna(subset=['text_clean', 'label_clean'])
final_data = final_data[final_data['label_clean'] != 'UNKNOWN']
final_data = final_data[final_data['text_clean'].str.len() > MIN_TEXT_LENGTH]

# L·ªçc ch·ªâ l·∫•y nh√£n A1-B2 (target c·ªßa d·ª± √°n)
final_data = final_data[final_data['label_clean'].isin(TARGET_LABELS)]

print("\nD·ªØ li·ªáu sau khi l√†m s·∫°ch v√† h·ª£p nh·∫•t:")
print(final_data.info())
print("\nPh√¢n b·ªï nh√£n cu·ªëi c√πng (A1-B2):")
print(final_data['label_clean'].value_counts())


# %%
# --- 2.5. TF-IDF v√† ph√¢n chia d·ªØ li·ªáu (70% Train / 10% Val / 20% Test) ---
print("\nƒêang th·ª±c hi·ªán TF-IDF v√† ph√¢n chia Train/Validation/Test...")

X = final_data['text_clean']
y = final_data['label_clean']

# Ph√¢n chia: 70% Train, 10% Validation, 20% Test
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(2/3), random_state=RANDOM_STATE, stratify=y_temp
)

print(f"S·ªë l∆∞·ª£ng m·∫´u Train:      {len(y_train):4d} (70%)")
print(f"S·ªë l∆∞·ª£ng m·∫´u Validation: {len(y_val):4d} (10%)")
print(f"S·ªë l∆∞·ª£ng m·∫´u Test:       {len(y_test):4d} (20%)")

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words='english')

X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_val_tfidf = tfidf_vectorizer.transform(X_val)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

print(f"K√≠ch th∆∞·ªõc ma tr·∫≠n TF-IDF (Train): {X_train_tfidf.shape}")
print("--- K·∫æT TH√öC CHU·∫®N B·ªä D·ªÆ LI·ªÜU ---")


# %%
# --- 2.6. Xu·∫•t d·ªØ li·ªáu v√†o th∆∞ m·ª•c dataframes ---
print("\n--- ƒêang xu·∫•t d·ªØ li·ªáu v√†o th∆∞ m·ª•c dataframes ---")

import os
from scipy.sparse import save_npz

# T·∫°o th∆∞ m·ª•c dataframes n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs('dataframes', exist_ok=True)

# 1. Xu·∫•t cleaned data (final_data)
final_data.to_csv('dataframes/cleaned_data.csv', index=False, encoding='utf-8')
print("‚úÖ ƒê√£ xu·∫•t: cleaned_data.csv")

# 2. Xu·∫•t train/val/test text v√† labels
# Train set
pd.DataFrame({
    'text': X_train.values,
    'label': y_train.values
}).to_csv('dataframes/train_data.csv', index=False, encoding='utf-8')
print("‚úÖ ƒê√£ xu·∫•t: train_data.csv")

# Validation set
pd.DataFrame({
    'text': X_val.values,
    'label': y_val.values
}).to_csv('dataframes/val_data.csv', index=False, encoding='utf-8')
print("‚úÖ ƒê√£ xu·∫•t: val_data.csv")

# Test set
pd.DataFrame({
    'text': X_test.values,
    'label': y_test.values
}).to_csv('dataframes/test_data.csv', index=False, encoding='utf-8')
print("‚úÖ ƒê√£ xu·∫•t: test_data.csv")

# 3. Xu·∫•t TF-IDF matrices (sparse format)
save_npz('dataframes/X_train_tfidf.npz', X_train_tfidf)
print("‚úÖ ƒê√£ xu·∫•t: X_train_tfidf.npz")

save_npz('dataframes/X_val_tfidf.npz', X_val_tfidf)
print("‚úÖ ƒê√£ xu·∫•t: X_val_tfidf.npz")

save_npz('dataframes/X_test_tfidf.npz', X_test_tfidf)
print("‚úÖ ƒê√£ xu·∫•t: X_test_tfidf.npz")

# 4. Xu·∫•t labels ri√™ng (d·∫°ng CSV)
pd.DataFrame({'label': y_train.values}).to_csv('dataframes/y_train.csv', index=False, encoding='utf-8')
print("‚úÖ ƒê√£ xu·∫•t: y_train.csv")

pd.DataFrame({'label': y_val.values}).to_csv('dataframes/y_val.csv', index=False, encoding='utf-8')
print("‚úÖ ƒê√£ xu·∫•t: y_val.csv")

pd.DataFrame({'label': y_test.values}).to_csv('dataframes/y_test.csv', index=False, encoding='utf-8')
print("‚úÖ ƒê√£ xu·∫•t: y_test.csv")

# 5. L∆∞u TF-IDF vectorizer ƒë·ªÉ t√°i s·ª≠ d·ª•ng
import pickle
with open('dataframes/tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf_vectorizer, f)
print("‚úÖ ƒê√£ xu·∫•t: tfidf_vectorizer.pkl")

print("\nüì¶ T·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c xu·∫•t v√†o th∆∞ m·ª•c 'dataframes/'")
print(f"   - cleaned_data.csv: {len(final_data)} d√≤ng")
print(f"   - train_data.csv: {len(y_train)} d√≤ng")
print(f"   - val_data.csv: {len(y_val)} d√≤ng")
print(f"   - test_data.csv: {len(y_test)} d√≤ng")
print(f"   - TF-IDF matrices: X_train, X_val, X_test (sparse .npz format)")
print(f"   - Labels: y_train, y_val, y_test (.csv format)")
print(f"   - TF-IDF vectorizer: tfidf_vectorizer.pkl")


# %%
# =============================================================================
# 3: SO S√ÅNH C√ÅC M√î H√åNH BASELINE
# =============================================================================
print("\n--- 3: SO S√ÅNH C√ÅC MODEL BASELINE ---")

# L·∫•y danh s√°ch nh√£n ƒë·ªÉ d√πng chung
labels_order = sorted(y.unique())


# %%
# --- Helper Function: V·∫Ω Confusion Matrix ---
def plot_confusion_matrix(y_true, y_pred, labels, title):
    """V·∫Ω confusion matrix cho model"""
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels)
    plt.title(title)
    plt.xlabel('Nh√£n D·ª± ƒëo√°n')
    plt.ylabel('Nh√£n Th·ª±c t·∫ø')
    plt.show()

# %%
# --- 3.a. Baseline model 1: Multinomial Naive Bayes ---
print("\n--- 3.a. Baseline model 1: Multinomial Naive Bayes ---")
print("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh Multinomial Naive Bayes...")
model_nb = MultinomialNB()
model_nb.fit(X_train_tfidf, y_train)
print("Hu·∫•n luy·ªán ho√†n t·∫•t.")

# ƒê√°nh gi√° tr√™n t·∫≠p Validation
print("ƒêang ƒë√°nh gi√° Naive Bayes tr√™n t·∫≠p Validation...")
y_pred_val_nb = model_nb.predict(X_val_tfidf)

accuracy_nb = accuracy_score(y_val, y_pred_val_nb)
print(f"\nƒê·ªô ch√≠nh x√°c (Accuracy) Naive Bayes: {accuracy_nb * 100:.2f}%")

print("\nB√°o c√°o Ph√¢n lo·∫°i (Naive Bayes) - T·∫≠p Validation:")
print(classification_report(y_val, y_pred_val_nb, labels=labels_order, zero_division=0))

print("\nƒêang v·∫Ω Ma tr·∫≠n Nh·∫ßm l·∫´n (Naive Bayes)...")
plot_confusion_matrix(y_val, y_pred_val_nb, labels_order, 
                      'Confusion Matrix (Naive Bayes) - T·∫≠p Validation')



# %%
# --- 3.b. Baseline model 2: Logistic Regression ---
print("\n--- 3.b. Baseline model 2: Logistic Regression ---")
print("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh Logistic Regression...")
model_lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)
model_lr.fit(X_train_tfidf, y_train)
print("Hu·∫•n luy·ªán ho√†n t·∫•t.")

# ƒê√°nh gi√° tr√™n t·∫≠p Validation
print("ƒêang ƒë√°nh gi√° Logistic Regression tr√™n t·∫≠p Validation...")
y_pred_val_lr = model_lr.predict(X_val_tfidf)

accuracy_lr = accuracy_score(y_val, y_pred_val_lr)
print(f"\nƒê·ªô ch√≠nh x√°c (Accuracy) Logistic Regression: {accuracy_lr * 100:.2f}%")

print("\nB√°o c√°o Ph√¢n lo·∫°i (Logistic Regression) - T·∫≠p Validation:")
print(classification_report(y_val, y_pred_val_lr, labels=labels_order, zero_division=0))

print("\nƒêang v·∫Ω Ma tr·∫≠n Nh·∫ßm l·∫´n (Logistic Regression)...")
plot_confusion_matrix(y_val, y_pred_val_lr, labels_order,
                      'Confusion Matrix (Logistic Regression) - T·∫≠p Validation')



# %%
# --- 3.c. Baseline model 3: SGDClassifier ---
print("\n--- 3.c. Baseline model 3: SGDClassifier ---")
print("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh SGDClassifier (hu·∫•n luy·ªán theo epoch)...")

# Kh·ªüi t·∫°o SGDClassifier
model_sgd = SGDClassifier(
    loss='log_loss', 
    random_state=RANDOM_STATE, 
    eta0=SGD_LEARNING_RATE, 
    learning_rate='adaptive'
)

val_losses = []  # L∆∞u validation loss sau m·ªói epoch
classes = labels_order

for epoch in range(SGD_N_EPOCHS):
    model_sgd.partial_fit(X_train_tfidf, y_train, classes=classes)
    
    # T√≠nh validation loss
    y_pred_val_prob_sgd = model_sgd.predict_proba(X_val_tfidf)
    val_loss = log_loss(y_val, y_pred_val_prob_sgd, labels=classes)
    val_losses.append(val_loss)
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch + 1}/{SGD_N_EPOCHS}, Validation Loss: {val_loss:.4f}")

print("Hu·∫•n luy·ªán SGD ho√†n t·∫•t.")

# V·∫Ω bi·ªÉu ƒë·ªì Loss Curve
print("\nƒêang v·∫Ω Loss Curve...")
plt.figure(figsize=(10, 6))
plt.plot(val_losses, label='Validation Loss')
plt.title('Loss Curve - SGDClassifier')
plt.xlabel('Epochs')
plt.ylabel('Log Loss')
plt.legend()
plt.grid(True)
plt.show()

# ƒê√°nh gi√° SGDClassifier
print("\nƒê√°nh gi√° SGDClassifier tr√™n t·∫≠p Validation...")
y_pred_val_sgd = model_sgd.predict(X_val_tfidf)

accuracy_sgd = accuracy_score(y_val, y_pred_val_sgd)
print(f"\nƒê·ªô ch√≠nh x√°c (Accuracy) SGDClassifier: {accuracy_sgd * 100:.2f}%")

print("\nB√°o c√°o Ph√¢n lo·∫°i (SGDClassifier) - T·∫≠p Validation:")
print(classification_report(y_val, y_pred_val_sgd, labels=labels_order, zero_division=0))

print("\nƒêang v·∫Ω Ma tr·∫≠n Nh·∫ßm l·∫´n (SGDClassifier)...")
plot_confusion_matrix(y_val, y_pred_val_sgd, labels_order,
                      'Confusion Matrix (SGDClassifier) - T·∫≠p Validation')


# %%
# --- 3.d. T·ªïng k·∫øt so s√°nh ---
print("\n--- 3.d. T·ªîNG K·∫æT SO S√ÅNH BASELINE ---")
print(f"Naive Bayes Accuracy:         {accuracy_nb * 100:.2f}%")
print(f"Logistic Regression Accuracy: {accuracy_lr * 100:.2f}%")
print(f"SGDClassifier Accuracy:       {accuracy_sgd * 100:.2f}%")

print("\n--- K·∫æT TH√öC ---")